{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14158afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict, Any, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import numpy as np\n",
    "from joblib import dump\n",
    "from sklearn.metrics import recall_score, precision_score, roc_auc_score, roc_curve, precision_recall_curve, f1_score\n",
    "\n",
    "from logger import logger\n",
    "\n",
    "\n",
    "def plot_roc(y_true: np.ndarray, y_pred: np.ndarray, model_name: str, plot_dir: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Plot the area under curve for the ROC curve.\n",
    "    Args:\n",
    "        y_true: Array of true y values\n",
    "        y_pred: Array of predicted y values\n",
    "        model_name: Name of model\n",
    "        plot_dir: Directory to save plot in\n",
    "    Returns:\n",
    "        Output path of plot\n",
    "    \"\"\"\n",
    "    fpr, tpr, roc_thresholds = roc_curve(y_true, y_pred)\n",
    "    plt.figure(figsize =(5, 5))\n",
    "    plt.grid()\n",
    "    plt.plot(fpr, tpr, color='b')\n",
    "    plt.title('ROC curve: {}'.format(model_name))\n",
    "\n",
    "    # Save figure\n",
    "    if plot_dir:\n",
    "        output_path = '{}/plots/roc_curve_{}.png'.format(plot_dir, model_name)\n",
    "        plt.savefig(output_path)\n",
    "        logger.info('ROC curve saved to: {}'.format(output_path))\n",
    "        return output_path\n",
    "\n",
    "\n",
    "def plot_precision_recall(y_true: np.ndarray, y_pred: np.ndarray, model_name: str, plot_dir: str = None) -> str:\n",
    "    \"\"\"\n",
    "    Plots the precision-recall curve.\n",
    "    Args:\n",
    "        y_true: Array of true y values\n",
    "        y_pred: Array of predicted y values\n",
    "        model_name: Name of model\n",
    "        plot_dir: Directory to save plot in\n",
    "    Returns:Ã˜\n",
    "        Output path of plot\n",
    "    \"\"\"\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_pred)\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.grid()\n",
    "    plt.plot(thresholds, precision[1:], color='r', label='Precision')\n",
    "    plt.plot(thresholds, recall[1:], color='b', label='Recall')\n",
    "    plt.gca().invert_xaxis()\n",
    "    plt.legend()\n",
    "    plt.title('Precision-recall curve: {}'.format(model_name))\n",
    "\n",
    "    # Save figure\n",
    "    if plot_dir:\n",
    "        output_path = '{}/plots/precision_recall_{}.png'.format(plot_dir, model_name)\n",
    "        plt.savefig(output_path)\n",
    "        logger.info('Precision-recall curve saved to: {}'.format(output_path))\n",
    "        return output_path\n",
    "\n",
    "\n",
    "def save_model(model: Any, model_name: str, model_dir: str) -> str:\n",
    "    \"\"\"\n",
    "    Saves model in pickle format\n",
    "    Args:\n",
    "        model: Model binary\n",
    "        model_name: Name of model\n",
    "        model_dir: Directory to save model in\n",
    "    Returns:\n",
    "        Output path of model\n",
    "    \"\"\"\n",
    "    output_path = '{}/models/{}.pickle'.format(model_dir, model_name)\n",
    "    logger.info('Model saved to: {}'.format(output_path))\n",
    "    dump(model, output_path)\n",
    "\n",
    "    return output_path\n",
    "\n",
    "\n",
    "def evaluate_binary(y_true: np.ndarray, y_pred: np.ndarray, threshold: float = 0.5) -> \\\n",
    "        Tuple[float, float, float, float]:\n",
    "    \"\"\"\n",
    "    Returns binary evaluation metrics\n",
    "    Args:\n",
    "        y_true: Array of true y values\n",
    "        y_pred: Array of predicted y values\n",
    "        threshold: Threshold to convert probabilies to binary values (default=0.5)\n",
    "    Returns:\n",
    "        Metrics for AUC, recall, precision, and F1\n",
    "    \"\"\"\n",
    "    y_pred_thresholded = np.where(y_pred > threshold, 1, 0)\n",
    "\n",
    "    auc = roc_auc_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred_thresholded)\n",
    "    precision = precision_score(y_true, y_pred_thresholded)\n",
    "    f1 = f1_score(y_true, y_pred_thresholded)\n",
    "\n",
    "    logger.info('AUC: {:.3f} | Recall: {:.3f} | Precision: {:.3f} | F1: {:.3f}'.format(auc, recall, precision, f1))\n",
    "    return auc, recall, precision, f1\n",
    "\n",
    "\n",
    "def log_mlflow(run_params: Dict, model: Any, model_name: str, y_true: np.ndarray, y_pred: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Logs result of model training and validation to mlflow\n",
    "    Args:\n",
    "        run_params: Dictionary containing parameters of run.\n",
    "                    Expects keys for 'experiment', 'artifact_dir', 'iteration', and 'index.\n",
    "        model: Model binary\n",
    "        model_name: Name of model\n",
    "        y_true: Array of true y values\n",
    "        y_pred: Array of predicted y values\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    mlflow.set_experiment(run_params['experiment'])\n",
    "\n",
    "    auc, recall, precision, f1 = evaluate_binary(y_true, y_pred)\n",
    "\n",
    "    roc_path = plot_roc(y_true, y_pred, '{} (auc = {:.2f})'.format(model_name, auc), run_params['artifact_dir'])\n",
    "    pr_path = plot_precision_recall(y_true, y_pred,\n",
    "                                    '{} (prec = {:.2f}, recall = {:.2f})'.format(model_name, precision, recall),\n",
    "                                    run_params['artifact_dir'])\n",
    "    model_path = save_model(model, model_name, run_params['artifact_dir'])\n",
    "\n",
    "    with mlflow.start_run(run_name=run_params['iteration']):\n",
    "        mlflow.log_param('index', run_params['index'])\n",
    "        mlflow.log_param('model', model_name)\n",
    "        mlflow.log_metric('auc', auc)\n",
    "        mlflow.log_metric('recall', recall)\n",
    "        mlflow.log_metric('precision', precision)\n",
    "        mlflow.log_metric('f1', f1)\n",
    "        mlflow.log_artifact(model_path)\n",
    "        mlflow.log_artifact(roc_path)\n",
    "        mlflow.log_artifact(pr_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c07ebb97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
